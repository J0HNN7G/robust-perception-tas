{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63002c7a-cf14-4ca4-92c3-1641ab2547ba",
   "metadata": {},
   "source": [
    "# Autonomous Perception Robustness Testing Framework (APRTF)\n",
    "### Development Journal\n",
    "\n",
    "We show that our general framework can be used on the [NuScenes](https://www.nuscenes.org/) dataset using a multi-stage analysis proposed in [\"Perception robustness testing at different levels of generality\"](https://www.journalfieldrobotics.org/FR/Papers_files/10_Pezzementi.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99d26d-689d-4e85-bcf2-b4cd5b6531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# augmentation\n",
    "import torchvision.transforms as T\n",
    "import aprtf.augmentations as A\n",
    "\n",
    "# dataset\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes import NuScenes\n",
    "data_dir = './data/sets/nuScenes'\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=data_dir, verbose=True)\n",
    "\n",
    "# torchvision reference code\n",
    "import aprtf.dataset as D\n",
    "from aprtf.torchvision_detection.coco_utils import get_coco_api_from_dataset\n",
    "from aprtf.pycocotools_robustness.cocoeval import COCOeval\n",
    "from aprtf.torchvision_detection.coco_eval import CocoEvaluator\n",
    "from aprtf.config import cfg\n",
    "from aprtf.models import ModelBuilder\n",
    "from aprtf.torchvision_detection import utils\n",
    "#from aprtf.analysis import Analyzer\n",
    "\n",
    "# logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"All packages imported!\")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Random seed set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b541a-2c58-40d8-98d3-96d9dca8e61a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I. Pedestrian Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d56c39-4d1f-477d-81d0-23ba42e07a70",
   "metadata": {},
   "source": [
    "### Data and Labels\n",
    "\n",
    "Time-ordered iterator of images and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuScenes\n",
    "def box2bb(box, cam_intrinsic):\n",
    "    corners = torch.tensor(view_points(box.corners(), view=cam_intrinsic, normalize=True)[:2, :])\n",
    "    bb = torch.cat([torch.min(corners, dim=1).values, torch.max(corners, dim=1).values]).tolist()\n",
    "    return bb\n",
    "    \n",
    "category = 'pedestrian'\n",
    "sensor = 'CAM_FRONT'\n",
    "visibility_threshold = 2\n",
    "\n",
    "odgt = []\n",
    "\n",
    "for scene in nusc.scene:\n",
    "    next_sample_token = scene['first_sample_token']\n",
    "    while next_sample_token:\n",
    "        sample = nusc.get('sample', next_sample_token)\n",
    "        sample_data = nusc.get('sample_data', sample['data'][sensor])\n",
    "\n",
    "        # image filepaths\n",
    "        sample_data_fp = os.path.join(data_dir,sample_data['filename'])\n",
    "\n",
    "        # bounding boxes\n",
    "        sample_data_bbs = []\n",
    "        for ann in sample['anns']:\n",
    "            _, box, cam_intrinsic = nusc.get_sample_data(sample['data'][sensor], selected_anntokens=[ann])\n",
    "            if len(box) > 1:\n",
    "                raise ValueError('more than one annotation')\n",
    "\n",
    "            visibility_token = nusc.get('sample_annotation', ann)['visibility_token']\n",
    "            visibility = int(visibility_token)\n",
    "            if (len(box) == 1) and (category in box[0].name) and (visibility >= visibility_threshold):\n",
    "                bb = box2bb(box[0], cam_intrinsic)\n",
    "                sample_data_bbs.append(bb)\n",
    "\n",
    "        # odgt\n",
    "        odgt.append(\n",
    "            {\n",
    "                'image': sample_data_fp,\n",
    "                'annotations': sample_data_bbs\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # next sample\n",
    "        next_sample_token = sample['next']\n",
    "\n",
    "normal_transform = D.get_transform(train=False) \n",
    "aug_transform = T.GaussianBlur(5,3)\n",
    "all_transform = A.TransformAugmentationCompose(normal_transform, aug_transform)\n",
    "\n",
    "dataset = D.PedestrianDetectionDataset(odgt, transforms=all_transform)\n",
    "\n",
    "config = 'retinanet_resnet50_fpn-pennfudanped'\n",
    "cfg_path = os.path.join('ckpt', config, 'config.yaml')\n",
    "cfg.merge_from_file(cfg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb58770-a621-4558-b319-dbe7a2d5643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'retinanet_resnet50_fpn-pennfudanped'\n",
    "cfg_path = os.path.join('ckpt', config, 'config.yaml')\n",
    "cfg.merge_from_file(cfg_path)\n",
    "\n",
    "#dataset_path = os.path.join('data','sets', 'PennFudanPed', cfg.DATASET.LIST.val)\n",
    "#dataset = D.PedestrianDetectionDataset(dataset_path, transforms=all_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0363c59-33f7-4c7f-a243-d9210b54e40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=1, shuffle=False, num_workers=1,\n",
    " collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01aefe-1375-4163-847e-6c6ce776856c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ddfb5-78e2-499b-b288-395c5a5c7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    logging.info('No GPU found! Training on CPU')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "weights_path = os.path.join('ckpt', config, 'weights_best.pth')\n",
    "model = ModelBuilder.build_detector(args=cfg.MODEL, weights=weights_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd503041-20ac-4353-af84-52f31347424c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metric\n",
    "\n",
    "Recall and $FPR_A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c3fc5-38bc-41f9-85b9-1adbc46f5b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    coco_evaluator = CocoEvaluator(COCOeval, coco, ['bbox'], score_min=0.9)\n",
    "\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        del images\n",
    "        # you need to do .item() because an int is not treated the same as a tensor int\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        coco_evaluator.update(res)\n",
    "\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320a748-b9eb-43a9-b8b9-7637b17f220a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_log = evaluate(model, data_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
