{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63002c7a-cf14-4ca4-92c3-1641ab2547ba",
   "metadata": {},
   "source": [
    "# Autonomous Perception Robustness Testing Framework (APRTF)\n",
    "### Development Journal\n",
    "\n",
    "We show that our general framework can be used on the [NuScenes](https://www.nuscenes.org/) dataset using a multi-stage analysis proposed in [\"Perception robustness testing at different levels of generality\"](https://www.journalfieldrobotics.org/FR/Papers_files/10_Pezzementi.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e99d26d-689d-4e85-bcf2-b4cd5b6531d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.425 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "All packages imported!\n",
      "Random Seed Set!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes import NuScenes\n",
    "data_dir = './data/sets/nuScenes'\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=data_dir, verbose=True)\n",
    "\n",
    "# torchvision reference code\n",
    "from aprtf.dataset import PedestrianDetectionDataset, get_transform\n",
    "from aprtf.references import utils\n",
    "from aprtf.visuals import visualize_results\n",
    "\n",
    "print(\"All packages imported!\")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print('Random Seed Set!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b541a-2c58-40d8-98d3-96d9dca8e61a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I. Pedestrian Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d56c39-4d1f-477d-81d0-23ba42e07a70",
   "metadata": {},
   "source": [
    "### Data and Labels\n",
    "\n",
    "Time-ordered iterator of images and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0c5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuScenes\n",
    "def box2bb(box, cam_intrinsic):\n",
    "    corners = torch.tensor(view_points(box.corners(), view=cam_intrinsic, normalize=True)[:2, :])\n",
    "    bb = torch.cat([torch.min(corners, dim=1).values, torch.max(corners, dim=1).values]).tolist()\n",
    "    return bb\n",
    "    \n",
    "category = 'pedestrian'\n",
    "sensor = 'CAM_FRONT'\n",
    "visibility_threshold = 2\n",
    "\n",
    "odgt = []\n",
    "\n",
    "for scene in nusc.scene:\n",
    "    next_sample_token = scene['first_sample_token']\n",
    "    while next_sample_token:\n",
    "        sample = nusc.get('sample', next_sample_token)\n",
    "        sample_data = nusc.get('sample_data', sample['data'][sensor])\n",
    "\n",
    "        # image filepaths\n",
    "        sample_data_fp = os.path.join(data_dir,sample_data['filename'])\n",
    "\n",
    "        # bounding boxes\n",
    "        sample_data_bbs = []\n",
    "        for ann in sample['anns']:\n",
    "            _, box, cam_intrinsic = nusc.get_sample_data(sample['data'][sensor], selected_anntokens=[ann])\n",
    "            if len(box) > 1:\n",
    "                raise ValueError('more than one annotation')\n",
    "\n",
    "            visibility_token = nusc.get('sample_annotation', ann)['visibility_token']\n",
    "            visibility = int(visibility_token)\n",
    "            if (len(box) == 1) and (category in box[0].name) and (visibility >= visibility_threshold):\n",
    "                bb = box2bb(box[0], cam_intrinsic)\n",
    "                sample_data_bbs.append(bb)\n",
    "\n",
    "        # odgt\n",
    "        odgt.append(\n",
    "            {\n",
    "                'image': sample_data_fp,\n",
    "                'annotations': sample_data_bbs\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # next sample\n",
    "        next_sample_token = sample['next']\n",
    "\n",
    "dataset = PedestrianDetectionDataset(odgt, get_transform(train=False))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=1, shuffle=False, num_workers=1,\n",
    " collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "images = []\n",
    "gt_bbs = []\n",
    "dt_bbs = [] \n",
    "for i in range(9):\n",
    "    output_ims, targets = next(iter(data_loader))\n",
    "    target = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    image = T.ToPILImage()(output_ims[0])\n",
    "    images.append(image)\n",
    "\n",
    "    gt_bbs.append(target[0]['boxes'])\n",
    "    dt_bbs.append(target[0]['boxes'] * 0.97)\n",
    "#visualize_results('output.png', images, gt_bbs, dt_bbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd503041-20ac-4353-af84-52f31347424c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metric\n",
    "\n",
    "Recall and $FPR_A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca322bc2-257e-4eab-b2df-275f11332760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall():\n",
    "    pass\n",
    "\n",
    "def FPRa():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
