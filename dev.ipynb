{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63002c7a-cf14-4ca4-92c3-1641ab2547ba",
   "metadata": {},
   "source": [
    "# Autonomous Perception Robustness Testing Framework (APRTF)\n",
    "### Development Journal\n",
    "\n",
    "We show that our general framework can be used on the [NuScenes](https://www.nuscenes.org/) dataset using a multi-stage analysis proposed in [\"Perception robustness testing at different levels of generality\"](https://www.journalfieldrobotics.org/FR/Papers_files/10_Pezzementi.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99d26d-689d-4e85-bcf2-b4cd5b6531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes import NuScenes\n",
    "data_dir = './data/sets/nuscenes'\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=data_dir, verbose=True)\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import torchvision.transforms as T\n",
    "# torchvision reference code\n",
    "from aprtf.references import utils, engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All packages imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b541a-2c58-40d8-98d3-96d9dca8e61a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I. Pedestrian Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d56c39-4d1f-477d-81d0-23ba42e07a70",
   "metadata": {},
   "source": [
    "### Data and Labels\n",
    "\n",
    "Time-ordered iterator of images and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4a7df-be67-4730-9fc2-8713dd7ceddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box2bb(box, cam_intrinsic):\n",
    "    corners = view_points(box.corners(), view=cam_intrinsic, normalize=True)[:2, :]\n",
    "    bb = np.concatenate([np.min(corners, axis=1), np.max(corners, axis=1)])\n",
    "    return bb\n",
    "\n",
    "def bb2rect(bb):\n",
    "    xmin, ymin, xmax, ymax = bb\n",
    "    corners = np.array([[xmin, ymin], \n",
    "                        [xmin, ymax],\n",
    "                        [xmax, ymax],\n",
    "                        [xmax, ymin]])\n",
    "    return corners\n",
    "\n",
    "def draw_rect(axis, selected_corners, color):\n",
    "    prev = selected_corners[-1]\n",
    "    for corner in selected_corners:\n",
    "        axis.plot([prev[0], corner[0]], [prev[1], corner[1]], color=color, linewidth=1)\n",
    "        prev = corner\n",
    "\n",
    "def render_annotations(axis, scene_data_fp, scene_data_bb):\n",
    "    im = Image.open(scene_data_fp)\n",
    "    axis.imshow(im)\n",
    "    axis.axis('off')\n",
    "    axis.set_aspect('equal')\n",
    "    for bb in scene_data_bb:\n",
    "        colors = ['b'] * 3\n",
    "        corners = bb2rect(bb)\n",
    "        draw_rect(axis, corners, colors[0])\n",
    "\n",
    "def render_results(axis, scene_data_fp, scene_data_bb, scene_data_pred):\n",
    "    im = Image.open(scene_data_fp)\n",
    "    axis.imshow(im)\n",
    "    axis.axis('off')\n",
    "    axis.set_aspect('equal')\n",
    "    \n",
    "    for bb in scene_data_bb:\n",
    "        colors = ['b'] * 3\n",
    "        corners = bb2rect(bb)\n",
    "        draw_rect(axis, corners, colors[0])\n",
    "        \n",
    "    for bb in scene_data_pred:\n",
    "        colors = ['orange'] * 3\n",
    "        corners = bb2rect(bb)\n",
    "        draw_rect(axis, corners, colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5777c42-6e9a-459e-9e81-3fb7d4919cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'pedestrian'\n",
    "sensor = 'CAM_FRONT'\n",
    "scene = nusc.scene[0]\n",
    "\n",
    "scene_data_fps = []\n",
    "scene_data_anns = []\n",
    "scene_data_bbs = []\n",
    "\n",
    "next_sample_token = scene['first_sample_token']\n",
    "while next_sample_token:\n",
    "    sample = nusc.get('sample', next_sample_token)\n",
    "    sample_data = nusc.get('sample_data', sample['data'][sensor])\n",
    "\n",
    "    # image filepaths\n",
    "    sample_data_fp = os.path.join(data_dir,sample_data['filename'])\n",
    "    scene_data_fps.append(sample_data_fp)\n",
    "\n",
    "    # bounding boxes\n",
    "    sample_data_bbs = []\n",
    "    sample_data_anns = []\n",
    "    for ann in sample['anns']:\n",
    "        _, box, cam_intrinsic = nusc.get_sample_data(sample['data'][sensor], selected_anntokens=[ann])\n",
    "        if len(box) > 1:\n",
    "            raise ValueError('more than one annotation')\n",
    "        elif (len(box) == 1) and (category in box[0].name):\n",
    "            bb = box2bb(box[0], cam_intrinsic)\n",
    "            sample_data_bbs.append(bb)\n",
    "            sample_data_anns.append(ann)\n",
    "    scene_data_bbs.append(sample_data_bbs)\n",
    "    scene_data_anns.append(sample_data_anns)\n",
    "    \n",
    "    next_sample_token = sample['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2299d-94ba-43a0-bc60-0c09086814ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 15\n",
    "fake_results = np.array(scene_data_bbs[idx]) + 20*np.random.rand(*np.shape(scene_data_bbs[idx]))\n",
    "\n",
    "# simple testing\n",
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "\n",
    "render_annotations(ax1, scene_data_fps[idx], scene_data_bbs[idx])\n",
    "render_results(ax2, scene_data_fps[idx], scene_data_bbs[idx], fake_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e20f4d-d1b0-4998-a205-addbbf03c69e",
   "metadata": {},
   "source": [
    "### Augmentations\n",
    "\n",
    "Composition of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c2750-05ac-4c45-9b75-39e65726916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_file = os.path.join(data_dir,sample_data_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfab9e-3187-47c3-9a1e-15e6ed750a37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model\n",
    "\n",
    "Takes in batch or single image, and outputs pedestrian bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2c829-3c91-4a98-8a23-3fb53a942661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2, fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "\n",
    "retinanet = retinanet_resnet50_fpn_v2(weights='DEFAULT')        \n",
    "# replace the pre-trained head with a new one\n",
    "in_features = retinanet.head.classification_head.cls_logits.in_channels\n",
    "num_anchors = retinanet.head.classification_head.num_anchors\n",
    "retinanet.head.classification_head = RetinaNetClassificationHead(in_features, num_classes, num_anchors)\n",
    "\n",
    "rcnn = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "# replace the pre-trained head with a new one\n",
    "in_features = rcnn.roi_heads.box_predictor.cls_score.in_features\n",
    "rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac741d06-d973-433d-b4c6-e1c142a2c382",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2aa1ca-1133-4e4a-85c1-5eae964bda46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PedestrianDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, boxes):\n",
    "        assert len(images) == len(boxes)\n",
    "        self.images = images\n",
    "        self.boxes = boxes\n",
    "        \n",
    "        # image processing\n",
    "        transforms = []\n",
    "        transforms.append(T.PILToTensor())\n",
    "        transforms.append(T.ConvertImageDtype(torch.float))\n",
    "        self.image_transforms = T.Compose(transforms)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # image\n",
    "        img_path = self.images[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.image_transforms(img)\n",
    "        \n",
    "        # boxes\n",
    "        bbs = self.boxes[idx]\n",
    "        bbs = torch.as_tensor(bbs, dtype=torch.float32)\n",
    "        bbs = torch.reshape(bbs, (-1,4))\n",
    "        \n",
    "        num_objs = len(bbs)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bbs\n",
    "        target[\"labels\"] = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bbs[..., 3] - bbs[..., 1]) * (bbs[..., 2] - bbs[..., 0])\n",
    "        target[\"iscrowd\"] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28103b-b5c8-49a8-a4ce-a1f2f03454ab",
   "metadata": {},
   "source": [
    "Forward test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31cc78-7e57-4b65-a18a-56eaecf15c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = rcnn\n",
    "dataset = PedestrianDetectionDataset(scene_data_fps, scene_data_bbs)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    " collate_fn=utils.collate_fn)\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)           # Returns predictions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00fb65-24e3-4686-862c-327457c3eb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random_set_splits(dataset_len, train_frac=0.8, test_frac=0.1, seed=42):\n",
    "    are_positive = (0 < train_frac) and (0 < test_frac)\n",
    "    in_range = (train_frac + test_frac <= 1)\n",
    "    if not (are_positive and in_range):\n",
    "        raise ValueError(\"Dataset splits are impossible\")\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = np.arange(dataset_len)\n",
    "    rand_idxs = rng.permutation(idxs)\n",
    "    \n",
    "    train_threshold = int(train_frac* dataset_len)\n",
    "    test_threshold = int((train_frac+test_frac) * dataset_len)\n",
    "    train, validate, test = np.split(rand_idxs, [train_threshold, test_threshold])\n",
    "    return train, validate, test\n",
    "\n",
    "def generate_set_splits(dataset_len, train_frac=0.8, test_frac=0.1):\n",
    "    are_positive = (0 < train_frac) and (0 < test_frac)\n",
    "    in_range = (train_frac + test_frac <= 1)\n",
    "    if not (are_positive and in_range):\n",
    "        raise ValueError(\"Dataset splits are impossible\")\n",
    "    \n",
    "    idxs = np.arange(dataset_len)\n",
    "    train_threshold = int(train_frac* dataset_len)\n",
    "    test_threshold = int((train_frac+test_frac) * dataset_len)\n",
    "    train, validate, test = np.split(idxs, [train_threshold, test_threshold])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c6c56-0ecb-46f1-a95a-56d35dfa10b2",
   "metadata": {},
   "source": [
    "Actual training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef8cbb-f5b5-42d8-b504-101a4f55cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model):\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = 2\n",
    "    \n",
    "    # use our dataset and defined transformations\n",
    "    dataset_train = PedestrianDetectionDataset(scene_data_fps, scene_data_bbs)\n",
    "    dataset_val = PedestrianDetectionDataset(scene_data_fps, scene_data_bbs)\n",
    "    dataset_test = PedestrianDetectionDataset(scene_data_fps, scene_data_bbs)\n",
    "    \n",
    "    train_idxs, val_idxs, test_idxs = generate_set_splits(len(scene_data_fps))\n",
    "\n",
    "    # split the dataset in train, val and test set\n",
    "    dataset_train = torch.utils.data.Subset(dataset_train, train_idxs)\n",
    "    dataset_val = torch.utils.data.Subset(dataset_val, val_idxs)\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, test_idxs)\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=1, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "    \n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # evaluate on the validation dataset\n",
    "        engine.evaluate(model, data_loader_val, device=device)\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        engine.train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "    # evaluation on test dataset\n",
    "    engine.evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7361f-d8cc-4930-82b4-3c31f4932349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main(rcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd503041-20ac-4353-af84-52f31347424c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metric\n",
    "\n",
    "Recall and $FPR_A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca322bc2-257e-4eab-b2df-275f11332760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall():\n",
    "    pass\n",
    "\n",
    "def FPRa():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd095aac-1d29-4a3f-a172-c580927858c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
