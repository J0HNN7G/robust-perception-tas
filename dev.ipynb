{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63002c7a-cf14-4ca4-92c3-1641ab2547ba",
   "metadata": {},
   "source": [
    "# Autonomous Perception Robustness Testing Framework (APRTF)\n",
    "### Development Journal\n",
    "\n",
    "We show that our general framework can be used on the [NuScenes](https://www.nuscenes.org/) dataset using a multi-stage analysis proposed in [\"Perception robustness testing at different levels of generality\"](https://www.journalfieldrobotics.org/FR/Papers_files/10_Pezzementi.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e99d26d-689d-4e85-bcf2-b4cd5b6531d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.445 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "All packages imported!\n",
      "Random seed set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/miniconda3/envs/tas/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/jonathan/miniconda3/envs/tas/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# augmentation\n",
    "import torchvision.transforms as T\n",
    "import aprtf.augmentations as A\n",
    "\n",
    "# dataset\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes import NuScenes\n",
    "data_dir = './data/sets/nuScenes'\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=data_dir, verbose=True)\n",
    "\n",
    "# torchvision reference code\n",
    "import aprtf.dataset as D\n",
    "from aprtf.torchvision_detection.coco_utils import get_coco_api_from_dataset\n",
    "from aprtf.pycocotools_robustness.cocoeval import COCOeval\n",
    "from aprtf.torchvision_detection.coco_eval import CocoEvaluator\n",
    "from aprtf.config import cfg\n",
    "from aprtf.models import ModelBuilder\n",
    "from aprtf.torchvision_detection import utils\n",
    "from aprtf.visuals import show_predictions, show_results\n",
    "#from aprtf.analysis import Analyzer\n",
    "\n",
    "# logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"All packages imported!\")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Random seed set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b541a-2c58-40d8-98d3-96d9dca8e61a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I. Pedestrian Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d56c39-4d1f-477d-81d0-23ba42e07a70",
   "metadata": {},
   "source": [
    "### Data and Labels\n",
    "\n",
    "Time-ordered iterator of images and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0c5181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# nuScenes\\ndef box2bb(box, cam_intrinsic):\\n    corners = torch.tensor(view_points(box.corners(), view=cam_intrinsic, normalize=True)[:2, :])\\n    bb = torch.cat([torch.min(corners, dim=1).values, torch.max(corners, dim=1).values]).tolist()\\n    return bb\\n    \\ncategory = 'pedestrian'\\nsensor = 'CAM_FRONT'\\nvisibility_threshold = 2\\n\\nodgt = []\\n\\nfor scene in nusc.scene:\\n    next_sample_token = scene['first_sample_token']\\n    while next_sample_token:\\n        sample = nusc.get('sample', next_sample_token)\\n        sample_data = nusc.get('sample_data', sample['data'][sensor])\\n\\n        # image filepaths\\n        sample_data_fp = os.path.join(data_dir,sample_data['filename'])\\n\\n        # bounding boxes\\n        sample_data_bbs = []\\n        for ann in sample['anns']:\\n            _, box, cam_intrinsic = nusc.get_sample_data(sample['data'][sensor], selected_anntokens=[ann])\\n            if len(box) > 1:\\n                raise ValueError('more than one annotation')\\n\\n            visibility_token = nusc.get('sample_annotation', ann)['visibility_token']\\n            visibility = int(visibility_token)\\n            if (len(box) == 1) and (category in box[0].name) and (visibility >= visibility_threshold):\\n                bb = box2bb(box[0], cam_intrinsic)\\n                sample_data_bbs.append(bb)\\n\\n        # odgt\\n        odgt.append(\\n            {\\n                'image': sample_data_fp,\\n                'annotations': sample_data_bbs\\n            }\\n        )\\n\\n        # next sample\\n        next_sample_token = sample['next']\\n\\ndataset = D.PedestrianDetectionDataset(odgt, get_transform(train=False))\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# nuScenes\n",
    "def box2bb(box, cam_intrinsic):\n",
    "    corners = torch.tensor(view_points(box.corners(), view=cam_intrinsic, normalize=True)[:2, :])\n",
    "    bb = torch.cat([torch.min(corners, dim=1).values, torch.max(corners, dim=1).values]).tolist()\n",
    "    return bb\n",
    "    \n",
    "category = 'pedestrian'\n",
    "sensor = 'CAM_FRONT'\n",
    "visibility_threshold = 2\n",
    "\n",
    "odgt = []\n",
    "\n",
    "for scene in nusc.scene:\n",
    "    next_sample_token = scene['first_sample_token']\n",
    "    while next_sample_token:\n",
    "        sample = nusc.get('sample', next_sample_token)\n",
    "        sample_data = nusc.get('sample_data', sample['data'][sensor])\n",
    "\n",
    "        # image filepaths\n",
    "        sample_data_fp = os.path.join(data_dir,sample_data['filename'])\n",
    "\n",
    "        # bounding boxes\n",
    "        sample_data_bbs = []\n",
    "        for ann in sample['anns']:\n",
    "            _, box, cam_intrinsic = nusc.get_sample_data(sample['data'][sensor], selected_anntokens=[ann])\n",
    "            if len(box) > 1:\n",
    "                raise ValueError('more than one annotation')\n",
    "\n",
    "            visibility_token = nusc.get('sample_annotation', ann)['visibility_token']\n",
    "            visibility = int(visibility_token)\n",
    "            if (len(box) == 1) and (category in box[0].name) and (visibility >= visibility_threshold):\n",
    "                bb = box2bb(box[0], cam_intrinsic)\n",
    "                sample_data_bbs.append(bb)\n",
    "\n",
    "        # odgt\n",
    "        odgt.append(\n",
    "            {\n",
    "                'image': sample_data_fp,\n",
    "                'annotations': sample_data_bbs\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # next sample\n",
    "        next_sample_token = sample['next']\n",
    "\n",
    "dataset = D.PedestrianDetectionDataset(odgt, get_transform(train=False))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb58770-a621-4558-b319-dbe7a2d5643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = os.path.join('ckpt', 'fasterrcnn_resnet50_fpn-pennfudanped', 'config.yaml')\n",
    "cfg.merge_from_file(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972f68d7-856b-4719-87b6-ff11266514cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join('data','sets', 'PennFudanPed', cfg.DATASET.LIST.val)\n",
    "\n",
    "normal_transform = D.get_transform(train=False) \n",
    "aug_transform = T.GaussianBlur(5,3)\n",
    "all_transform = A.TransformAugmentationCompose(normal_transform, aug_transform)\n",
    "\n",
    "dataset = D.PedestrianDetectionDataset(dataset_path, transforms=all_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0363c59-33f7-4c7f-a243-d9210b54e40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=1, shuffle=False, num_workers=1,\n",
    " collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01aefe-1375-4163-847e-6c6ce776856c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73ddfb5-78e2-499b-b288-395c5a5c7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for detector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    logging.info('No GPU found! Training on CPU')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "weights_path = os.path.join('ckpt', 'fasterrcnn_resnet50_fpn-pennfudanped', 'weights_best.pth')\n",
    "model = ModelBuilder.build_detector(args=cfg.MODEL, weights=weights_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd503041-20ac-4353-af84-52f31347424c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metric\n",
    "\n",
    "Recall and $FPR_A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387c3fc5-38bc-41f9-85b9-1adbc46f5b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    coco_evaluator = CocoEvaluator(COCOeval, coco, ['bbox'], score_min=0.9)\n",
    "\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        del images\n",
    "        # you need to do .item() because an int is not treated the same as a tensor int\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        coco_evaluator.update(res)\n",
    "\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3320a748-b9eb-43a9-b8b9-7637b17f220a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:17<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_log = evaluate(model, data_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d4087f-9484-445d-ba6c-52b7412ba2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<aprtf.torchvision_detection.coco_eval.CocoEvaluator at 0x7fbf1a7ab790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46991807-59a6-4830-bb27-c856cdd8bae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 0.001, 0.01, 0.1, 1, 10, 100] [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# sensitivity, i.e., predicted fpr on validation set\n",
    "scores = np.arange(0,1.01,0.01)\n",
    "# compute fpr at each score range, match each score to closest predicted fpr \n",
    "sens = np.ones_like(scores) * -1.0\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    fpr = 0\n",
    "    sens[i] = fpr\n",
    "    \n",
    "std_sens = [10 ** x for x in [-4, -3, -2, -1, 0, 1, 2]]\n",
    "std_score = [scores[(np. abs(x - sens)).argmin()] for x in std_sens]\n",
    "print(std_sens, std_score)\n",
    "\n",
    "\n",
    "# loop through all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca322bc2-257e-4eab-b2df-275f11332760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sROCa():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e97662-0cfa-4fbe-a8b0-2d6c4d267c46",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e70747b5-5d78-4f56-90ea-180a55093cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = [dt1, dt2, dt3, dt4]\\naugs = [tf1, None, None, None] \\nmodel = [detector, robotizer, pathifier, controller]\\nmetric = [roc, dist_acc, safety_violation, speed_efficiency]\\n\\nanalyzer = Analyzer(data, augs, model, metric)\\nfor i in range(len(analyzer)):\\n    result = analyzer.step()\\n    print(result)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data = [dt1, dt2, dt3, dt4]\n",
    "augs = [tf1, None, None, None] \n",
    "model = [detector, robotizer, pathifier, controller]\n",
    "metric = [roc, dist_acc, safety_violation, speed_efficiency]\n",
    "\n",
    "analyzer = Analyzer(data, augs, model, metric)\n",
    "for i in range(len(analyzer)):\n",
    "    result = analyzer.step()\n",
    "    print(result)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
